{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, silhouette_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n",
    "                              GradientBoostingClassifier)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data=pd.read_pickle(\"process_data/clean_data/sandp_ta.plk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\power\\Dropbox\\Cory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean quarterly financial data\n",
    "\n",
    "quarterly_fin=pd.read_csv(\"SP500_quarterly_fin.zip\")\n",
    "\n",
    "#vars to deal with\n",
    "\n",
    "date_vars=['datadate','fyearq','fqtr','fyr']\n",
    "\n",
    "num_vars=['acchgq','acoq','actq','altoq','ancq','anoq',\\\n",
    "'aociderglq','aociotherq','aocisecglq','aol2q','aoq','apq','aqaq',\\\n",
    "'aqdq','aqepsq','aqpl1q','aqpq','arcedq','arceepsq','arceq','atq',\\\n",
    "'aul3q','billexceq','capr1q','capr2q','capsftq','capsq','capxy',\\\n",
    "'cdvcy','chechy','ceiexbillq','ceqq','cheq','chq','cibegniq','cicurrq',\\\n",
    "'ciderglq','cimiiq','ciotherq','ciy','cogsq','cshfd12','cshfdq','cshiq',\\\n",
    "'cshopq','cshoq','cshprq','cshtrq','cstkeq','cstkq','dcomq','dd1q','deracq',\\\n",
    "'deraltq','derhedglq','derlcq','derlltq','diladq','dlcq','dlttq','doq',\\\n",
    "'dpacreq','dpactq','dpq','dpretq','drcq','drltq','dteaq','dtedq','dteepsq',\\\n",
    "'dtepq','dvintfq','dvpq','dvpspq','epsfiq','epsfxy','esoptq','esubq','fcaq',\\\n",
    "'ffoq','finacoq','finchq','findlcq','findltq','finivstq','finltoq','finreccq',\\\n",
    "'finrecltq','finrevq','finxintq','finxoprq','gdwlamq','gdwliaq','gdwlidq',\\\n",
    "'gdwliepsq','gdwlipq','gdwlq','glaq','glceaq','glceepsq','glcepq','glivq',\\\n",
    "'glpq', 'hedgeglq','ibq','icaptq','intaccq','intanoq','intanq','invfgq','invtq',\\\n",
    "'itccy','ivaeqq','ivltq','ivstq','lctq','lltq','lnoq','lol2q','lul3q','ltq','mibnq',\\\n",
    "'mibq','mibtq','miiq','mkvaltq','msaq','ncoq','niitq','nimq','niq','nopiq','npatq',\\\n",
    "'npq','nrtxtepsq','nrtxtq','obkq','oepsxq','oiadpq','opepsq','piq','pllq',\\\n",
    "'pncepsq','pncpq','pstkq','rcaq','rcdq','recdq','recdq','rdipq','rectq','req',\\\n",
    "'revtq','rllq','rraq','saleq','setaq','spceq','spiq','tiiq','tieq','txdbaq','txdbclq',\\\n",
    "'txdbq','txwq','uaptq','uceqq','wcapq','wdaq','xaccq','xiq']\n",
    "\n",
    "cat_vars=['gvkey','tic','cusip','idbflag','loc','naics','state']\n",
    "\n",
    "#drop those missing columns\n",
    "quarterly_info=quarterly_fin[num_vars].describe().T\n",
    "num_filtered=list(quarterly_info[quarterly_info['count']>13000].index)\n",
    "\n",
    "#data filtered\n",
    "quarterly_fin_filtered=quarterly_fin[date_vars+num_filtered+cat_vars]\n",
    "quarterly_fin_filtered.loc[quarterly_fin_filtered['state'].isnull(),'state']=quarterly_fin_filtered['loc']\n",
    "quarterly_fin_filtered.fillna(0, inplace=True)\n",
    "quarterly_fin_filtered['year']=np.floor(quarterly_fin_filtered['datadate']/10000)\n",
    "quarterly_fin_filtered['quarter']=np.floor((quarterly_fin_filtered['datadate']-quarterly_fin_filtered['year']*10000)/100)//3+quarterly_fin_filtered['year']*100\n",
    "\n",
    "#dealing with dummies\n",
    "states=pd.get_dummies(quarterly_fin_filtered['state'])\n",
    "states.columns=[\"state_\"+str(i) for i in states.columns]\n",
    "\n",
    "naics=pd.get_dummies(quarterly_fin_filtered['naics'])\n",
    "naics.columns=[\"naics_\"+str(i) for i in naics.columns]\n",
    "\n",
    "idbflag=pd.get_dummies(quarterly_fin_filtered['idbflag'])\n",
    "idbflag.columns=[\"idbflag_\"+str(i) for i in idbflag.columns]\n",
    "\n",
    "#save the data\n",
    "quarterly_fin_filtered=pd.concat([quarterly_fin_filtered, states, naics,idbflag], axis=1).drop(['idbflag','loc','naics','state'], axis=1)\n",
    "\n",
    "quarterly_fin_filtered.to_pickle(\"quarterly_spy.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.dropbox',\n",
       " 'company_financial_info.zip',\n",
       " 'desktop.ini',\n",
       " 'execucomp',\n",
       " 'Execucomp_Data_Definitions.pdf',\n",
       " 'Fundamentals Quarterly Manual.html',\n",
       " 'Fundamentals Quarterly Manual_files',\n",
       " 'new.pkl',\n",
       " 'quarterly_spy.pkl',\n",
       " 'quarterly_spy_w_inst',\n",
       " 'quarterly_spy_w_inst.pickle',\n",
       " 'Security Daily Manual.html',\n",
       " 'Security Daily Manual_files',\n",
       " 'SP500_daily.zip',\n",
       " 'SP500_quarterly_fin.zip',\n",
       " 'stock_institutional_ownership.csv',\n",
       " 'ticker_prices.zip']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deal with institutional ownership data\n",
    "institutional=pd.read_csv(\"stock_institutional_ownership.csv\", encoding='latin1')\n",
    "\n",
    "institutional_vars=['quarter','TIC','mktcap','io_usd', 'nbr_firms', 'io', \\\n",
    "                    'io_dom', 'io_for', 'io_for_us','io_for_nus', 'io_dom_indep', \\\n",
    "                    'io_dom_grey', 'io_for_indep',\\\n",
    "                    'io_for_grey', 'io_cat1', 'io_cat2', 'io_cat3', 'io_cat4', 'io_cat5',\\\n",
    "                    'io_cat6', 'io_cat7', 'io_indep', 'io_grey', 'io_common', 'io_civil',\\\n",
    "                    'io_for_common', 'io_for_civil', 'ibh_5pct', 'ibh_1pct', 'top5','herf']\n",
    "\n",
    "institutional_filtered=institutional[institutional_vars]\n",
    "\n",
    "institutional_filtered.rename(columns={'TIC':'tic'}, inplace=True)\n",
    "\n",
    "institutional_filtered.drop_duplicates(['tic','quarter'], inplace=True)\n",
    "\n",
    "#merge with financial data\n",
    "fin_and_inst=quarterly_fin_filtered.merge(institutional_filtered, on=['tic','quarter'])\n",
    "\n",
    "fin_and_inst.to_pickle(\"quarterly_spy_w_inst.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#merge everything with stock prices\n",
    "\n",
    "quarterly_fin_filtered=pd.read_pickle(\"quarterly_spy_w_inst.pickle\")\n",
    "\n",
    "quarterly_fin_filtered['date']=pd.to_datetime(quarterly_fin_filtered['datadate'].astype(str),format='%Y%m%d')\n",
    "quarterly_fin_filtered.set_index(['date'], inplace=True)\n",
    "quarterly_fin_to_merge=quarterly_fin_filtered.groupby('tic').apply(lambda x: x.asfreq(\"D\"))\n",
    "merged=stock_data.drop('tic',axis=1).join(quarterly_fin_filled.drop('tic',axis=1), how='left')\n",
    "\n",
    "#since the frequency is not the same across all the data, after merge, we ffill na values\n",
    "merged=merged.groupby('tic', as_index=False).fillna(method='ffill')\n",
    "merged=merged.reset_index()\n",
    "merged_filled.dropna(inplace=True)\n",
    "\n",
    "merged_filled.to_pickle(\"new.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some trial code for technical notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\power\\Dropbox\\Cory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing and rescaling\n",
    "\n",
    "full_data=pd.read_pickle(\"full_data.pkl\")\n",
    "\n",
    "X = full_data.drop('roi_class', axis=1)\n",
    "y = full_data.roi_class\n",
    "\n",
    "X_poly=pd.concat([X.loc[:,'30 period CCI_30':\"MFV_\"], X.loc[:,'30 period CCI_30_sandp':]], axis=1)\n",
    "X_dummy=X.loc[:,'state_AL':'idbflag_D']\n",
    "X_normal=X.drop((list(X_poly.columns)+list(X_dummy.columns)), axis=1)\n",
    "\n",
    "sd = StandardScaler()\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "Xp = poly.fit_transform(X_poly)\n",
    "X_to_scale=pd.concat([pd.DataFrame(Xp),X_Normal], axis=1)\n",
    "X_scaled=sd.fit_transform(X_to_scale)\n",
    "X_joined=pd.concat([pd.DataFrame(X_scaled), X_dummy], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_joined, y, test_size=0.2)\n",
    "X_train, X_train_lr, y_train, y_train_lr = train_test_split(\n",
    "    X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest and logit ensembling and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\power\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\power\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\power\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\power\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=7, n_estimators=300)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_lm = LogisticRegression(solver='lbfgs')\n",
    "rf_enc = OneHotEncoder(categories='auto')\n",
    "rf.fit(X_train, y_train)\n",
    "rf_enc.fit(rf.apply(X_train))\n",
    "rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n",
    "\n",
    "RF_Logit_ensemble_test = rf_lm.score(rf_enc.transform(rf.apply(X_test)),y_test)\n",
    "RF_Logit_ensemble_train = rf_lm.score(rf_enc.transform(rf.apply(X_train)),y_train)\n",
    "print(RF_Logit_ensemble_train, RF_Logit_ensemble_test)\n",
    "\n",
    "#deal with svc\n",
    "gammas = [0.1, 0.5, 1, 5, 10]\n",
    "score_dict={}\n",
    "for gamma in gammas:\n",
    "   svc = SVC(kernel='rbf', gamma=gamma).fit(X_train, y_train)\n",
    "   score_dict[gamma]=(y_test, svc.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some grid search cross validation for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline to search for the best combination of PCA truncation\n",
    "# and classifier regularization.\n",
    "logistic = SGDClassifier(loss='log', penalty='l2', early_stopping=True,\n",
    "                         max_iter=10000, tol=1e-5, random_state=0)\n",
    "pca = PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "# Parameters of pipelines can be set using '__' separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': [50, 100, 150, 200, 300, 400, 600],\n",
    "    'logistic__alpha': np.logspace(-10, 10, 5),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, iid=False, cv=3)\n",
    "search.fit(X_joined, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(X_joined)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance')\n",
    "\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# For each number of components, find the best classifier results\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "components_col = 'param_pca__n_components'\n",
    "best_clfs = results.groupby(components_col).apply(\n",
    "    lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "               legend=False, ax=ax1)\n",
    "ax1.set_ylabel('Classification accuracy (val)')\n",
    "ax1.set_xlabel('n_components')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
